{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-means.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1i/08y0xHEXefkUzxMV+u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsU353875gXK",
        "outputId": "af84129b-56fe-4d81-bf61-4cfc427c8b14"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-7jk4zyvfI"
      },
      "source": [
        "import operator\n",
        "import sys\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVzWsAbsyzww"
      },
      "source": [
        "# Macros.\n",
        "MAX_ITER = 20\n",
        "DATA_PATH = \"gs://apt2141-hw1-bucket/data.txt\"\n",
        "C1_PATH = \"gs://apt2141-hw1-bucket/c1.txt\"\n",
        "C2_PATH = \"gs://apt2141-hw1-bucket/c2.txt\"\n",
        "NORM = 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57yEtYqfy1zB"
      },
      "source": [
        "# Helper functions.\n",
        "def closest(p, centroids, norm):\n",
        "    \"\"\"\n",
        "    Compute closest centroid for a given point.\n",
        "    Args:\n",
        "        p (numpy.ndarray): input point\n",
        "        centroids (list): A list of centroids points\n",
        "        norm (int): 1 or 2\n",
        "    Returns:\n",
        "        int: The index of closest centroid.\n",
        "    \"\"\"\n",
        "    closest_c = min([(i, linalg.norm(p - c, norm))\n",
        "                    for i, c in enumerate(centroids)],\n",
        "                    key=operator.itemgetter(1))[0]\n",
        "    return closest_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44GB-X1gy5ju"
      },
      "source": [
        "# K-means clustering\n",
        "def kmeans(data, centroids, norm=2):\n",
        "    \"\"\"\n",
        "    Conduct k-means clustering given data and centroid.\n",
        "    This is the basic version of k-means, you might need more\n",
        "    code to record cluster assignment to plot TSNE, and more\n",
        "    data structure to record cost.\n",
        "    Args:\n",
        "        data (RDD): RDD of points\n",
        "        centroids (list): A list of centroids points\n",
        "        norm (int): 1 or 2\n",
        "    Returns:\n",
        "        RDD: assignment information of points, a RDD of (centroid, (point, 1))\n",
        "        list: a list of centroids\n",
        "        and define yourself...\n",
        "    \"\"\"\n",
        "    # iterative k-means\n",
        "    for _ in range(MAX_ITER):\n",
        "        # Transform each point to a combo of point, closest centroid, count=1\n",
        "        # point -> (closest_centroid, (point, 1))\n",
        "\n",
        "        # Re-compute cluster center\n",
        "        # For each cluster center (key), aggregate its values\n",
        "        # by summing up points and count\n",
        "        \n",
        "        # Average the points for each centroid: divide sum of points by count\n",
        "\n",
        "        # Use collect() to turn RDD into list\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71jwGlR_r9bc",
        "outputId": "5870faab-bfad-4d24-d02b-dd5f19e0cc70"
      },
      "source": [
        "# Spark settings\n",
        "conf = SparkConf()\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Load the data, cache this since we're accessing this each iteration\n",
        "data = sc.textFile(DATA_PATH).map(\n",
        "        lambda line: np.array([float(x) for x in line.split(' ')])\n",
        "        ).cache()\n",
        "# Load the initial centroids c1, split into a list of np arrays\n",
        "centroids1 = sc.textFile(C1_PATH).map(\n",
        "        lambda line: np.array([float(x) for x in line.split(' ')])\n",
        "        ).collect()\n",
        "# Load the initial centroids c2, split into a list of np arrays\n",
        "centroids2 = sc.textFile(C2_PATH).map(\n",
        "        lambda line: np.array([float(x) for x in line.split(' ')])\n",
        "        ).collect()\n",
        "\n",
        "# TODO: Run the kmeans clustering and complete the HW\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 63 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=b8ab87f0b51ecb181c87d78e7a88b1ff1b3849e66fce0b12ec40a8d29060fdf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    }
  ]
}