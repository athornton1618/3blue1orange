{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Reconstruction Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports - Do Not Modify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/lm/anaconda3/lib/python3.6/site-packages/general_utils/tf_estimator_hooks.py:9: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Force TF to only take the GPU memory it needs, because this can cause\n",
    "# distributed training to hang for certain GPU/model combinations.\n",
    "# We must do this prior to importing tensorflow.\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "\n",
    "# Enable Tensorflow XLA optimization. This should introduce some overhead\n",
    "# onto the first step of a train call, but measurably speed up training\n",
    "# beyond. This chooses different kernels and/or consolidates kernels to\n",
    "# optimize the graph.\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import tf_logging\n",
    "\n",
    "# Load used functions from autoencoders\n",
    "from autoencoders.autoencoder import get_train_and_eval_input_functions, \\\n",
    "                                     get_autoencoder\n",
    "from autoencoders.score_hackathon_models import save_model, \\\n",
    "                                                score_model\n",
    "import autoencoders.synthetic_data_params as synthetic_data_params\n",
    "\n",
    "from autoencoders.post_training_utils import predict_on_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7ff371892f60>: Failed to establish a new connection: [Errno 113] No route to host',))': /simple/torch/\u001b[0m\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-dac180b75103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m pip install torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters\n",
    "User-defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_dir            = \"/domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-bi-gru-14/\"\n",
    "learning_rate        = 1e-3\n",
    "batch_size           = 512\n",
    "latent_dimension     = \"easy\" # {\"easy\", \"hard\"}\n",
    "total_training_steps = 1280*5\n",
    "steps_per_validation = 640\n",
    "force_training       = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-106-15c01bd173aa>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-106-15c01bd173aa>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    layer = tf.keras.layers.\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def mfc_embed(layer):\n",
    "    #input is bsz x 64 x 128 x 1, w == time? \n",
    "    \n",
    "    layer = tf.keras.layers.\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Autoencoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "PUT YOUR ENCODER MODEL HERE!!  \n",
    "This function is passed a single argument, layer, which contains the images for this batch. The shape is (batch_size, 64, 128, 1) i.e. (batch_size, H, W, num_channels). The function must return a Tensor with shape (batch_size, X). This tensor is then passed to a fully connected layer to get the latent features (outside of this function).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder( layer ):\n",
    "\n",
    "    # Conv Layers\n",
    "    #layer = tf.keras.layers.Conv.ConvLSTM2D(filters = 16, kernel_size=3)(layer)\n",
    "    #layer = tf.keras.layers.Conv2D(filters=64, kernel_size=3)(layer)\n",
    "    \n",
    "    \n",
    "    layerCNN = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3)(layer)\n",
    "    layerCNN = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(layerCNN)\n",
    "    \n",
    "    layerCNN = tf.keras.layers.Flatten()(layerCNN)\n",
    "    \n",
    "    layerGRU = tf.keras.layers.GRU(units=256)(tf.squeeze(input=layer, axis=3))\n",
    "    \n",
    "    \n",
    "    layerSiam = tf.keras.layers.concatenate([layerGRU, layerCNN])\n",
    "    layerEnd = tf.keras.layers.Flatten()(layerSiam)\n",
    "    layerEnd  = tf.keras.layers.Dense(units=512)(layerSiam)\n",
    "        \n",
    "    '''\n",
    "    layer = tf.squeeze(input=layer, axis=3)\n",
    "        \n",
    "    layer = tf.keras.layers.GRU(units=512)(layer)\n",
    "    \n",
    "    #layer = tf.keras.layers.MaxPool1D(pool_size=(2))(layer)\n",
    "    layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(tf.expand_dims(layer, axis=2))\n",
    "    \n",
    "    layer = tf.keras.layers.Flatten()(layer)\n",
    "    \n",
    "    layer = tf.keras.layers.Dense(units=512)(layer)\n",
    "    '''\n",
    "    '''\n",
    "    layer1 = tf.layers.conv2d( layer,\n",
    "                              filters=16,\n",
    "                              kernel_size=3,\n",
    "                              strides=(1, 1),\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"encoder_layer_conv_sia1\" )\n",
    "    layer2 = tf.layers.conv2d( layer,\n",
    "                              filters=32,\n",
    "                              kernel_size=5,\n",
    "                              strides=(2, 2),\n",
    "                              padding=tf.SAME,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"encoder_layer_conv_sia2\" )\n",
    "\n",
    "    \n",
    "    layer = tf.layers.average_pooling1d(tf.convert_to_tensor([layer1, layer2]),\n",
    "                              filters=64,\n",
    "                              kernel_size=3,\n",
    "                              strides=(1, 1),\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"encoder_layer_conv2\" )\n",
    "    \n",
    "    # MaxPooling Layer\n",
    "    layer = tf.layers.max_pooling2d( layer,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     name=\"encoder_layer_maxpool\" )\n",
    "\n",
    "    # Flatten layer.    \n",
    "    layer = tf.layers.flatten( layer,\n",
    "                               name=\"encoder_layer_flatten\" )\n",
    "    \n",
    "    # Dense layer.\n",
    "    layer = tf.layers.dense( layer,\n",
    "                             units=512,\n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"encoder_layer_dense1\" )\n",
    "'''\n",
    "    #layer = tf.keras.layers.ConvLSTM2D(activation=tf.nn.relu, name=\"encoder_layer_convlstm1\", )\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder\n",
    "\n",
    "PUT YOUR DECODER MODEL HERE!!  \n",
    "This function is passed a single argument, layer, which contains\n",
    "the latent features for this batch. The shape is (batch_size, N) \n",
    "where N is either 64 or 8, depending on the size of the latent layer.\n",
    "The function must return a Tensor with shape (batch_size, 64, 128, 1),\n",
    "which is the same shape as the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder( layer ):\n",
    "    \n",
    "    layer = tf.keras.layers.Dense(units=512)(layer)\n",
    "    layer = tf.keras.layers.Reshape(target_shape = (-1, 4, 128))(layer)\n",
    "    layer = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                            kernel_size=3, strides=(2, 2))(layer)\n",
    "    layer = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                            kernel_size=3, strides=(1, 1))(layer)\n",
    "    layer = tf.image.resize(layer, (64, 128))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    layer = tf.keras.layers.GRU(units=512)(tf.expand_dims(layer, axis=2))\n",
    "    \n",
    "    layer = tf.keras.layers.Reshape(target_shape=(4, 128, 1))(layer)\n",
    "\n",
    "    layer = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2))(layer)\n",
    "    \n",
    "    layer = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1))(layer)\n",
    "    \n",
    "    layer = tf.image.resize( layer, (64, 128) )\n",
    "\n",
    "    \n",
    "    # Dense layer.\n",
    "    layer = tf.layers.dense( layer,\n",
    "                             units=512,\n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"decoder_layer_dense1\" )\n",
    "    \n",
    "    # Reshape\n",
    "    layer = tf.reshape( layer, (-1,\n",
    "                                4,\n",
    "                                128,\n",
    "                                1) )\n",
    "    \n",
    "    # Conv Transpose layers.\n",
    "    layer = tf.layers.conv2d_transpose( layer,\n",
    "                                        filters=64,\n",
    "                                        kernel_size=3,\n",
    "                                        strides=(2, 2),\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name=\"decoder_layer_conv1\" )\n",
    "    \n",
    "    layer = tf.layers.conv2d_transpose( layer,\n",
    "                                        filters=1,\n",
    "                                        kernel_size=3,\n",
    "                                        strides=(1, 1),\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name=\"decoder_layer_conv2\" )\n",
    "    \n",
    "    # Resize to match output shape. Note that this function does\n",
    "    # bilinear interpolation.\n",
    "    layer = tf.image.resize( layer, (64, 128) )\n",
    "    '''\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Definition Function \n",
    "\n",
    "Modify with caution!\n",
    "\n",
    "This function defines the full end-to-end autoencoder architecture. Your ENCODER and DECODER function are used here.\n",
    "  \n",
    "This can be modified if you want, but we recommend implementing your model through the above ENCODER and DECODER functions.\n",
    "\n",
    "\n",
    "The user-defined parameters are exposed here as the \"params\" dictionary. If you decide to touch this function, note that it must return different outputs depending on the model (TRAIN, EVAL, PREDICT). These correspond with the estimator.train(), estimator.eval(), and estimator.predict() function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_model_def_func( features,\n",
    "                         labels,\n",
    "                         mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                         params=None ):\n",
    "    \n",
    "    if( isinstance( features, dict ) ):\n",
    "        features = features[\"images\"]\n",
    "    \n",
    "        \n",
    "    # Reshape\n",
    "    input_layer = tf.reshape( features, (-1,\n",
    "                                         64,\n",
    "                                         128,\n",
    "                                         1) )\n",
    "    \n",
    "    # 1. Get the encoder\n",
    "    encoder_layers = encoder( input_layer )\n",
    "    \n",
    "    # 2. Add in the latent layer (depends on 'easy' or 'hard)\n",
    "    if( params[\"latent_dimension\"] == \"easy\" ):\n",
    "        num_latent_dims = 64\n",
    "    elif( params[\"latent_dimension\"] == \"hard\" ):\n",
    "        num_latent_dims = 8\n",
    "        \n",
    "    latent_layer = tf.layers.dense(\n",
    "            encoder_layers,\n",
    "            units=num_latent_dims,\n",
    "            activation=None,\n",
    "            name=\"latent_layer_units_{}\".format( num_latent_dims ) )\n",
    "    \n",
    "    # 3. Get the decoder\n",
    "    decoder_layers = decoder( latent_layer )\n",
    "    \n",
    "    # Flatten layer\n",
    "    # Needs to be (batch_size, 8192) to match the input.\n",
    "    decoder_layers = tf.layers.flatten( decoder_layers,\n",
    "                               name=\"decoder_final_flatten\" )\n",
    "    \n",
    "    # If predicting, labels are None by default.\n",
    "    # To calculate rmse, set labels to features.\n",
    "    if( tf.estimator.ModeKeys.PREDICT == mode ):\n",
    "        labels = features\n",
    "        \n",
    "    # 4. Compute loss and backprop\n",
    "    squared_error        = tf.pow( labels - decoder_layers, 2 )\n",
    "    # NOTE: Average across pixels.\n",
    "    rmse                 = tf.sqrt( tf.reduce_mean( input_tensor=squared_error, axis=1 ) )\n",
    "    reconstruction_error = tf.reduce_mean( input_tensor=rmse )\n",
    "    \n",
    "    if( tf.estimator.ModeKeys.PREDICT == mode ):\n",
    "        \n",
    "        # Create prediction outputs.\n",
    "        predictions = {}\n",
    "        predictions[\"latent_features\"] = latent_layer\n",
    "\n",
    "        predictions[\"recon\"]         = decoder_layers\n",
    "        predictions[\"squared_error\"] = squared_error\n",
    "        predictions[\"rmse\"]          = rmse\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec( mode, predictions=predictions )\n",
    "\n",
    "    # Loss is reconstruction error.\n",
    "    loss = reconstruction_error\n",
    "    \n",
    "    if( mode == tf.estimator.ModeKeys.EVAL ):\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss )\n",
    "    \n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer( params[\"learning_rate\"] )\n",
    "    train_op  = optimizer.minimize( loss,\n",
    "                                    global_step=tf.train.get_global_step() )\n",
    "    \n",
    "    \n",
    "    estimator = tf.estimator.EstimatorSpec( mode,\n",
    "                                            loss=loss,\n",
    "                                            train_op=train_op )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-bi-gru-14/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1200, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2eeae1cb38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4130 autoencoder 18:29:28 WARNING: model directory is not empty, continuing from a previous run due to FORCE_TRAINING being True: /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-bi-gru-14/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-bi-gru-14/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-bi-gru-14/model.ckpt.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[cluster_1083_1/merge_oidx_0/_173]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[cluster_1083_1/merge_oidx_0/_173]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-c0b0d7ca9298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     full_ae.train( input_fn=training_input_fn,\n\u001b[0;32m---> 56\u001b[0;31m                    steps=current_train_steps )\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdo_eval\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1260\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         logging.info(\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/lm/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[cluster_1083_1/merge_oidx_0/_173]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: Allocating 8589934592 bytes exceeds the memory limit of 4294967296 bytes.\n\t [[{{node cluster_1085_1/xla_compile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# Define dictionary to hold the relevant parameters for the autoencoder definition function.\n",
    "params = {\n",
    "   \"learning_rate\": learning_rate,\n",
    "   \"latent_dimension\": latent_dimension\n",
    "}\n",
    "    \n",
    "# Get AE class    \n",
    "ae_obj = get_autoencoder( full_model_def_func, # Function defining your model\n",
    "                          params, # User-defined parameters. Can be any dictionary\n",
    "                          model_dir, # Directory where that model will be saved.\n",
    "                          force_training ) # If False, model will not train on an existing\n",
    "                                           # checkpoint. Set to True to continue training\n",
    "                                           # an existing model\n",
    "    \n",
    "# Get training and validation data. The training_input_fn will generate on infinite\n",
    "# stream of synthetic data for the model to train on.\n",
    "## DO NOT TOUCH\n",
    "(training_input_fn,\n",
    " eval_input_fn,\n",
    " validation_images,\n",
    " validation_labels_argmax,\n",
    " classifier_params ) = get_train_and_eval_input_functions( model_dir,\n",
    "                                                           False, #make_feature_detector,\n",
    "                                                           None, #detector_feature_class,\n",
    "                                                           False, #use_transfer_learning,\n",
    "                                                           batch_size,\n",
    "                                                           512, #DEFAULT_VALIDATION_SET_SIZE,\n",
    "                                                           1, #validation_seed,\n",
    "                                                           None, #training_seed,\n",
    "                                                           0, #dali_device_id, default value\n",
    "                                                           2, #dali_prefetch_batches, default value\n",
    "                                                           False ) #use_mlpp_preprocessing\n",
    "    \n",
    "full_ae = ae_obj.full_network()\n",
    "    \n",
    "# Training loop\n",
    "training_steps_run = 0\n",
    "  \n",
    "start_time = time.time()\n",
    "  \n",
    "while training_steps_run < total_training_steps:\n",
    "        \n",
    "    # Determine if we can do an eval.\n",
    "    train_steps_remaining = total_training_steps - training_steps_run\n",
    "    do_eval               = train_steps_remaining >= steps_per_validation\n",
    "        \n",
    "    # Determine how many steps we need to train this time.\n",
    "    if( do_eval ):\n",
    "        current_train_steps = steps_per_validation\n",
    "    else:\n",
    "        current_train_steps = train_steps_remaining\n",
    "        \n",
    "    training_steps_run += current_train_steps\n",
    "        \n",
    "    full_ae.train( input_fn=training_input_fn,\n",
    "                   steps=current_train_steps )\n",
    "       \n",
    "    if( do_eval ):\n",
    "            \n",
    "        # Do eval.\n",
    "        eval_result = full_ae.evaluate( input_fn=eval_input_fn,\n",
    "                                        steps=1 )\n",
    "    \n",
    "        # Each step save a montage for the first 100 validation images\n",
    "        predict_on_data( ae_obj,\n",
    "                         validation_images[:100, :],\n",
    "                         synthetic_data_params.IMAGE_SHAPE,\n",
    "                         model_dir,\n",
    "                         labels=validation_labels_argmax[:100],\n",
    "                         image_min=synthetic_data_params.IMAGE_MIN,\n",
    "                         image_max=synthetic_data_params.IMAGE_MAX,\n",
    "                         delta_image_min=synthetic_data_params.DELTA_IMAGE_MIN,\n",
    "                         delta_image_max=synthetic_data_params.DELTA_IMAGE_MAX,\n",
    "                         data_description_string=\"validation\" )\n",
    "\n",
    "        # Save model\n",
    "        save_model( model_dir, full_ae )\n",
    "            \n",
    "# Get the training time.\n",
    "final_training_time = time.time() - start_time\n",
    "print( \"Train Time: {}\".format( final_training_time ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions and Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-gru-13/latest_model/variables/variables\n",
      "Validation score for model /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-gru-13/ is 1.0364092588424683\n",
      "Validation took 0.780996561050415 seconds\n"
     ]
    }
   ],
   "source": [
    "score, score_time = score_model( model_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an example function for how to load an existing model and use it to predict on data.\n",
    "def post_processing( model_dir, \n",
    "                     validation_images ):\n",
    "\n",
    "    # Load the model checkpoint\n",
    "    predict_fn = tf.contrib.predictor.from_saved_model( model_dir + \"/latest_model\")\n",
    "    \n",
    "    # Predict on the images\n",
    "    prediction_dict = predict_fn( {\"images\": validation_images} )\n",
    "    \n",
    "    # Look at the results\n",
    "    print( prediction_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /domino/datasets/daniel_adams/oct_hackathon_wc/scratch/vae-lstm-6//latest_model/variables/variables\n",
      "{'rmse': array([1.0943036 , 1.025197  , 1.0825996 , 0.9256538 , 1.0511086 ,\n",
      "       1.1801039 , 1.0847465 , 1.1178546 , 0.89692014, 1.0375382 ,\n",
      "       0.9273722 , 1.0944893 , 0.96832776, 0.96083087, 1.0204191 ,\n",
      "       1.3616666 , 0.995985  , 1.0452601 , 0.99799305, 0.8742171 ,\n",
      "       1.0550975 , 0.86653996, 1.1654879 , 1.0191513 , 1.2582949 ,\n",
      "       0.96122855, 1.203408  , 1.070215  , 1.0757774 , 1.0080407 ,\n",
      "       1.4533216 , 0.96032965, 1.0687022 , 0.9899083 , 1.0675678 ,\n",
      "       1.2646142 , 0.929964  , 1.1120285 , 1.0158714 , 1.0043011 ,\n",
      "       1.2209396 , 1.1801454 , 1.034436  , 0.9959754 , 0.9674092 ,\n",
      "       0.9856183 , 1.3288122 , 1.118889  , 0.99115   , 0.95166904,\n",
      "       0.9698147 , 1.1066239 , 1.0129445 , 1.0454241 , 1.2125835 ,\n",
      "       1.1491838 , 1.0653809 , 1.0425754 , 1.036703  , 0.98953915,\n",
      "       0.92558306, 1.0524768 , 1.115516  , 0.94090766, 1.1325694 ,\n",
      "       1.080341  , 0.983772  , 1.1788251 , 0.96429944, 0.93657434,\n",
      "       0.9639424 , 1.2939464 , 1.0434303 , 1.2350967 , 1.0238332 ,\n",
      "       1.0918394 , 1.021632  , 0.93310976, 1.2205203 , 1.0115936 ,\n",
      "       1.0174451 , 0.9555519 , 0.9928464 , 1.082082  , 1.0090022 ,\n",
      "       1.1966561 , 1.1795794 , 1.063658  , 0.8561702 , 1.0049291 ,\n",
      "       1.1177264 , 0.890241  , 1.111198  , 1.159278  , 1.1830262 ,\n",
      "       1.039724  , 0.962006  , 1.1129699 , 0.9865704 , 0.8979699 ,\n",
      "       1.1278771 , 1.2864239 , 1.1742004 , 0.92808247, 1.1643877 ,\n",
      "       1.0460044 , 1.0865881 , 0.9483143 , 1.0999643 , 1.1858268 ,\n",
      "       1.0368782 , 1.2854146 , 1.0487214 , 1.0285038 , 0.91618395,\n",
      "       1.0633317 , 1.0242345 , 0.9810196 , 1.0766135 , 1.0220704 ,\n",
      "       1.1657373 , 1.0417329 , 0.92172235, 0.9095168 , 1.2077684 ,\n",
      "       1.0753717 , 1.0172852 , 0.9858901 , 1.0953816 , 1.3123842 ,\n",
      "       1.0028625 , 0.9617223 , 1.038867  , 1.0385759 , 1.1487482 ,\n",
      "       1.1741915 , 0.9180033 , 1.3778142 , 0.9508191 , 0.9504389 ,\n",
      "       1.1811465 , 1.260923  , 1.1608241 , 1.1579028 , 1.0948232 ,\n",
      "       1.0347801 , 1.091979  , 1.044728  , 1.2004901 , 1.1903135 ,\n",
      "       1.5004402 , 1.0745696 , 1.13297   , 1.0955526 , 1.0527341 ,\n",
      "       1.1823878 , 0.9964962 , 0.9463166 , 1.2722445 , 0.97390395,\n",
      "       1.2894274 , 1.0213753 , 1.0152538 , 1.1394426 , 1.100745  ,\n",
      "       1.1265925 , 0.89754665, 1.2661946 , 1.1096705 , 1.0491713 ,\n",
      "       1.0369002 , 1.2473189 , 1.100333  , 1.0239817 , 1.0289614 ,\n",
      "       0.9924275 , 0.98252565, 1.0706474 , 1.0051123 , 0.9940989 ,\n",
      "       0.9826166 , 1.1183158 , 1.0630734 , 0.99420995, 0.926331  ,\n",
      "       1.1247637 , 1.1817102 , 1.0366585 , 1.0133228 , 1.1787399 ,\n",
      "       1.002486  , 1.0053438 , 1.0478594 , 1.297997  , 0.9468786 ,\n",
      "       1.1534402 , 1.1015569 , 0.9747958 , 0.9615922 , 1.1534246 ,\n",
      "       1.0752234 , 1.20831   , 1.1986578 , 0.9855974 , 1.093244  ,\n",
      "       1.3145125 , 1.1022241 , 1.0129488 , 1.1763093 , 1.1497785 ,\n",
      "       1.0505853 , 1.0286169 , 1.0022647 , 1.0321033 , 1.0899158 ,\n",
      "       0.97956145, 0.9174268 , 0.9525011 , 1.0203719 , 1.2351397 ,\n",
      "       0.95084715, 0.91039896, 0.92410564, 0.9337997 , 1.060745  ,\n",
      "       0.9719859 , 0.90557206, 0.95783895, 0.93516046, 0.84089196,\n",
      "       0.9335481 , 1.1849138 , 1.1115746 , 0.8944961 , 1.2119571 ,\n",
      "       1.0574743 , 1.0049149 , 1.2635372 , 1.1713915 , 1.0700091 ,\n",
      "       0.920988  , 0.98087776, 1.1323348 , 1.3804468 , 0.90904534,\n",
      "       1.0317744 , 1.1268767 , 1.1759268 , 1.0093732 , 0.94273305,\n",
      "       1.0116137 , 0.98341745, 0.98869455, 0.983663  , 0.9265285 ,\n",
      "       1.0525669 , 0.9820211 , 1.0908324 , 1.322383  , 0.8742596 ,\n",
      "       1.05009   , 1.0946475 , 0.9679773 , 1.2512383 , 1.030403  ,\n",
      "       0.968294  , 1.0467405 , 1.0636119 , 1.071312  , 0.94885653,\n",
      "       1.1099924 , 1.0831312 , 0.98255235, 1.1310991 , 1.0725234 ,\n",
      "       1.0349059 , 1.0918014 , 0.9639736 , 1.0261118 , 0.99745536,\n",
      "       1.1626121 , 1.3164852 , 0.9568352 , 1.193415  , 0.8887143 ,\n",
      "       1.1644341 , 1.1427042 , 0.9693702 , 1.1404265 , 1.1203709 ,\n",
      "       0.9380524 , 1.0561928 , 1.0132693 , 0.99359864, 0.9529526 ,\n",
      "       1.201093  , 1.086455  , 1.1429608 , 1.0015244 , 0.97224295,\n",
      "       1.1290228 , 1.2262468 , 1.0448368 , 0.9526093 , 1.0037252 ,\n",
      "       1.1496762 , 0.97534597, 0.99500996, 1.1889131 , 1.2284491 ,\n",
      "       0.8236611 , 1.1016723 , 1.0330572 , 1.0571791 , 1.0800205 ,\n",
      "       1.0079633 , 1.2061802 , 0.91824913, 0.9249104 , 1.0183889 ,\n",
      "       1.1800888 , 0.99196476, 0.9573633 , 0.96283466, 0.938828  ,\n",
      "       1.0164453 , 1.2172375 , 1.1038525 , 1.0035548 , 1.1978195 ,\n",
      "       0.99034476, 0.9871855 , 1.2026436 , 0.99162596, 1.0897595 ,\n",
      "       1.0548643 , 1.2125742 , 1.1025473 , 1.0222143 , 1.0626916 ,\n",
      "       0.97038513, 1.1483836 , 1.1784868 , 1.0028955 , 1.1849434 ,\n",
      "       0.86199975, 0.88097334, 0.92840534, 1.0564494 , 0.9418816 ,\n",
      "       1.0695722 , 1.0725297 , 1.0346447 , 1.1773728 , 0.96574205,\n",
      "       1.0923665 , 0.9227618 , 1.0897954 , 1.1058841 , 1.1945505 ,\n",
      "       1.1992124 , 0.94324094, 0.9391072 , 1.1799928 , 0.9901599 ,\n",
      "       0.8974913 , 1.4306071 , 1.1158583 , 1.0344287 , 1.0538317 ,\n",
      "       1.137807  , 1.0412856 , 1.068781  , 1.0500978 , 1.1510181 ,\n",
      "       1.0317785 , 1.0798403 , 1.0257902 , 1.2345523 , 0.9626282 ,\n",
      "       1.1644856 , 0.97369313, 1.0923282 , 1.0158888 , 0.9528074 ,\n",
      "       0.95303875, 0.96707344, 1.0908852 , 1.1534499 , 1.3381054 ,\n",
      "       1.1556703 , 0.92839664, 1.0128675 , 1.0642501 , 1.0170013 ,\n",
      "       1.1755319 , 0.9688876 , 1.1743051 , 1.1116937 , 1.1627353 ,\n",
      "       1.0906068 , 0.93357134, 1.1047583 , 1.1760012 , 1.1090038 ,\n",
      "       1.0603484 , 1.0305892 , 1.1385392 , 1.2128786 , 1.1426483 ,\n",
      "       0.99552673, 0.99273115, 1.0117766 , 1.1744168 , 0.92208725,\n",
      "       1.0292736 , 1.2231896 , 1.0047095 , 1.0409411 , 1.0779083 ,\n",
      "       1.0367398 , 0.9712134 , 1.0315807 , 1.0690049 , 0.9893263 ,\n",
      "       0.89103633, 0.911977  , 1.0704167 , 0.9832095 , 1.0113331 ,\n",
      "       0.9350431 , 1.1248225 , 1.0156362 , 1.1186696 , 1.1876668 ,\n",
      "       1.1051722 , 0.9213778 , 1.2140191 , 1.3640457 , 0.92854875,\n",
      "       1.1153704 , 0.9705376 , 1.0215733 , 0.86694866, 1.0215302 ,\n",
      "       1.1650293 , 1.1056687 , 0.97770995, 1.1454947 , 1.020538  ,\n",
      "       1.0830431 , 0.9660737 , 0.92216694, 1.0381424 , 1.3288587 ,\n",
      "       0.9716801 , 1.1307806 , 1.3387787 , 1.2367742 , 1.1099577 ,\n",
      "       1.0305817 , 0.9225948 , 1.0792367 , 1.1273155 , 1.0526204 ,\n",
      "       0.9764645 , 1.0420848 , 0.96934   , 1.3207384 , 0.9561168 ,\n",
      "       0.99173033, 1.2718049 , 0.9739337 , 1.1434854 , 0.940203  ,\n",
      "       0.924539  , 1.1230776 , 0.925531  , 1.1380831 , 1.0018964 ,\n",
      "       1.2675545 , 1.4272873 , 1.2159104 , 1.2760986 , 1.0630406 ,\n",
      "       1.034687  , 0.9397347 , 0.9976755 , 0.9940899 , 1.2505547 ,\n",
      "       0.98781365, 1.2835057 , 1.0384494 , 1.1707935 , 0.92078036,\n",
      "       0.91497785, 0.97952056, 1.151579  , 0.97117895, 1.1286385 ,\n",
      "       0.98230183, 0.89483553, 1.0544599 , 1.0661846 , 1.0293931 ,\n",
      "       1.0496427 , 1.2367015 , 1.1797936 , 1.1224182 , 1.0644318 ,\n",
      "       1.146915  , 1.2184284 ], dtype=float32), 'recon': array([[-0.03331617, -0.00475566,  0.02023132, ...,  0.00292976,\n",
      "        -0.01079721, -0.01202649],\n",
      "       [-0.03308708, -0.00436599,  0.02063565, ..., -0.07376973,\n",
      "        -0.05961784, -0.05835051],\n",
      "       [-0.0435078 , -0.02209099, -0.0035272 , ..., -0.02083646,\n",
      "        -0.02592486, -0.02638054],\n",
      "       ...,\n",
      "       [-0.01501253,  0.02637774,  0.06357992, ...,  0.05650723,\n",
      "         0.02330583,  0.02033258],\n",
      "       [-0.01620076,  0.02435663,  0.06106448, ...,  0.06894777,\n",
      "         0.03122447,  0.02784626],\n",
      "       [-0.01081335,  0.0335203 ,  0.07465743, ...,  0.10241967,\n",
      "         0.05252996,  0.04806222]], dtype=float32), 'latent_features': array([[ 0.58686894,  1.5160482 ,  0.2252139 , ..., -0.16923603,\n",
      "        -0.85696167, -2.008898  ],\n",
      "       [ 1.3237685 ,  1.5425007 ,  0.7237307 , ..., -0.54948586,\n",
      "        -2.620423  , -1.4085981 ],\n",
      "       [-0.03719275,  0.24222417, -0.87138987, ..., -0.387592  ,\n",
      "         0.87250614, -3.192246  ],\n",
      "       ...,\n",
      "       [ 0.27560663,  0.7383951 ,  0.31754753, ...,  1.4744445 ,\n",
      "        -0.87582886,  2.0836864 ],\n",
      "       [ 1.2520614 ,  1.7300088 ,  0.9803436 , ...,  0.6478622 ,\n",
      "        -1.1581936 , -0.55448925],\n",
      "       [ 1.4838593 ,  1.6139762 ,  1.1866704 , ..., -0.50269175,\n",
      "        -1.904523  ,  0.8204317 ]], dtype=float32), 'squared_error': array([[1.01341205e-02, 1.43252537e-01, 1.03751850e+00, ...,\n",
      "        5.63397333e-02, 1.75415456e-01, 7.08205998e-01],\n",
      "       [6.60159945e-01, 3.07754129e-01, 1.15051174e+00, ...,\n",
      "        5.49303591e-01, 2.44612060e-02, 1.43497037e-02],\n",
      "       [4.70389652e+00, 2.62434483e+00, 3.07001233e-01, ...,\n",
      "        8.31129313e-01, 1.29159644e-01, 4.23610151e-01],\n",
      "       ...,\n",
      "       [2.43927217e+00, 9.40375589e-03, 3.16852868e-01, ...,\n",
      "        4.70115207e-02, 8.22081044e-02, 6.82659924e-01],\n",
      "       [8.10335994e-01, 2.25209808e+00, 8.14604640e-01, ...,\n",
      "        4.13640976e-01, 4.65383530e-01, 3.26506607e-02],\n",
      "       [1.16519690e+00, 2.64211372e-02, 2.00123549e+00, ...,\n",
      "        2.41511822e+01, 2.32582474e+01, 1.16965187e+00]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "post_processing(model_dir, validation_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
